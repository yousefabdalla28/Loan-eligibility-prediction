# -*- coding: utf-8 -*-
"""Loan_Eligibility_Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fVf2YcHGWXI0RzN3n5PVgDwTrwEx_E1T

# Introduction:
The loan eligibility prediction project aims to automate the loan eligibility process by identifying customer segments eligible for loan amounts, based on customer details such as gender, marital status, education, income, loan amount, credit history, and others. The dataset consists of 615 rows, each consisting of 12 features and 1 dependent feature (loan status) with two classes, Y (yes eligible) or N (no not eligible). In this report, we will discuss the preprocessing techniques, analysis and visualizations, models and hyperparameters used, and other techniques applied to enhance the results.
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd  
import numpy as np   
import seaborn as sns  
import matplotlib.pyplot as plt  
# %matplotlib inline 
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_selection import RFE
import warnings
warnings.filterwarnings("ignore")

"""# Preprocessing

Before building the models, we ensured that the dataset was clean and ready-to-use by dealing with empty cells or duplicate records, converting the used categorical columns (gender, marital status, education, property area) to numerical columns using one of the encoding techniques (label encoding), applying feature scaling (normalization) for variables, and modifying data in the wrong format if found.
"""

data = pd.read_csv("loan_data_set.csv")

data.head()

data.shape

data.describe()

data.isnull().sum()

data['Credit_History'] =data['Credit_History'].fillna(data['Credit_History'].mode()[0])
data['Self_Employed'] =data['Self_Employed'].fillna(data['Self_Employed'].mode()[0])

data=data.dropna() 
#to avoid introducing too much noise into the model.

data

data.isnull().sum()

data.duplicated().sum()

data["Dependents"].value_counts()

data["Dependents"] = data["Dependents"].replace(to_replace="3+",value=3)

data["Dependents"].value_counts()

"""# Visualizations

---


"""

data.hist(figsize=(12,15),edgecolor='black');

sns.countplot(x='Credit_History',hue='Loan_Status',data=data)

sns.countplot(x='Gender',hue='Loan_Status',data=data)

sns.countplot(x='Married',hue='Loan_Status',data=data)

sns.countplot(x='Education',hue='Loan_Status',data=data)

sns.countplot(x='Property_Area',hue='Loan_Status',data=data)

sns.countplot(x="Dependents",hue="Loan_Status",data=data)

sns.boxplot(data=data,palette='rainbow',orient='h')
#to show outliers

"""# handleing outliers"""

# Define the columns to check for outliers
cols = ['ApplicantIncome', 'CoapplicantIncome','LoanAmount']

# Calculate the IQR for each column
Q1 = data[cols].quantile(0.25)
Q3 = data[cols].quantile(0.75)
IQR = Q3 - Q1

# Define the upper and lower bounds for outliers
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Replace the outliers with Q1 or Q3 values
for col in cols:
    data.loc[data[col] < lower_bound[col], col] = Q1[col]
    data.loc[data[col] > upper_bound[col], col] = Q3[col]

print(data['Loan_Amount_Term'].value_counts())

sns.boxplot(data=data,palette='rainbow',orient='h')
#to show outliers

data.info()

le = LabelEncoder()

# apply LabelEncoder to categorical columns
data['Gender'] = le.fit_transform(data['Gender'])
data['Married'] = le.fit_transform(data['Married'])
data['Education'] = le.fit_transform(data['Education'])
data['Self_Employed'] = le.fit_transform(data['Self_Employed'])
data['Property_Area'] = le.fit_transform(data['Property_Area'])
data['Loan_Status'] = le.fit_transform(data['Loan_Status'])
data['Dependents'].replace({'0':0,'1':1,'2':2,'3':3}, inplace=True)

data.head()
data.info()

"""# Feature Selection and Extraction:
To visualize correlation, we plotted a correlation matrix heatmap using the Seaborn library. We observed that credit history had the highest correlation with loan status, followed by marital status.but it did not result in significant improvements in the model performance.
"""

#Generating a correlation matrix
corr_matrix = data.corr(numeric_only=True)
plt.figure(figsize=(16,5))
sns.heatmap(corr_matrix, annot=True)
plt.show()

print(corr_matrix["Loan_Status"].sort_values(ascending=False))

"""**Credit_History** has high correlation as it provides information about the borrower's creditworthiness and ability to repay the loan.

# Feature Selection
"""

X  = data.drop(columns=["Loan_ID","Loan_Status"],axis=1)
X

Y = data["Loan_Status"]
Y

from sklearn.feature_selection import SelectKBest, chi2
# perform chi-squared feature selection
selector = SelectKBest(chi2, k=5)
X = selector.fit_transform(X, Y)

# print the selected features
print("Selected features:", selector.get_support())

# create a dataframe from the numpy array
X = pd.DataFrame(X, columns=['Married', 'ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Credit_History'])
X

Y

"""# Feature Scaling (Standardization)

to normalize the range of values of the input features or independent variables in a machine learning model.

This can be useful for improving the performance of many machine learning algorithms

standardize the data, by subtracting the mean of each column and dividing by its standard deviation. This ensures that each column has a mean of 0 and a standard deviation of 1.
"""

cols = ['Married', 'ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Credit_History']

from sklearn.preprocessing import StandardScaler 
st = StandardScaler()
X[cols]=st.fit_transform(X[cols])
X

"""# Model Training & Model Evaluation

We trained three models, namely Logistic Regression, SVM, and Decision Tree (ID3), using the Scikit-learn library.We split the data into 80% training and 20% testing sets.

We evaluated the models on the testing set using various evaluation metrics such as accuracy, confusion matrix, and classification report. The accuracy of the Logistic Regression model was 0.875, SVM was 0.875, and Decision Tree was 0.77 . The confusion matrix and classification report showed that Logistic Regression performed the best, with higher precision and recall scores for both classes.
"""

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score ,f1_score,confusion_matrix, classification_report, mean_squared_error
from sklearn.model_selection import RandomizedSearchCV

model_df = {}
def model_val(model, X, Y):
    X_train, X_test, Y_train, Y_test = train_test_split(X, Y,
                                                        test_size=0.20,  #20% test
                                                        random_state=42) #random data in this state
    model.fit(X_train, Y_train) 
    Y_pred = model.predict(X_test)
    print(f"{model} accuracy is {f1_score(Y_test, Y_pred)}")
    print(f"{model} confusion matrix is\n{confusion_matrix(Y_test, Y_pred)}")
    print(f"{model} classification report is\n{classification_report(Y_test, Y_pred)}")
    print(f"{model} mean squared error is {mean_squared_error(Y_test, Y_pred)}\n")

"""# Logistic Regression

"""

from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
model_val(model,X,Y)

"""**Confusion Matrix for the Logistic Regression shows that:**

 the model correctly predicted 12 samples as not eligible and 77 samples as eligible, but incorrectly predicted 22 samples as eligible (false positives) and 0 samples as not eligible (false negatives).

**Precision is the ratio of true positives to the total predicted positives (TP / (TP + FP))**

The precision for class 0 is 1.00, which means that all the samples predicted as not eligible by the model were actually not eligible.

 The precision for class 1 is 0.78, which means that 78% of the samples predicted as eligible by the model were actually eligible.

**Recall: Recall is the ratio of true positives to the total actual positives (TP / (TP + FN)).**

 In this case, the recall for class 0 is 0.35, which means that only 35% of the actual not eligible samples were correctly identified by the model. The recall for class 1 is 1.00, which means that all the actual eligible samples were correctly identified by the model.

Logistic Regression model has a higher precision score for class 1 (eligible) than both the SVC and Decision Tree models. This means that the Logistic Regression model is better at correctly predicting eligible loan applicants.

**Hyperparameter Tuning**  
 
 the process of selecting the best combination of hyperparameters for a machine learning model that maximizes its performance
"""

log_reg_grid={"C":np.logspace(-4,4,20),# function generates a sequence of 20 numbers that are logarithmically spaced between 1e-4 and 1e4,
             "solver":['liblinear']} # 'liblinear' solver is a good choice for small datasets and binary classification problems.

rs_log_reg=RandomizedSearchCV(LogisticRegression(),
                   param_distributions=log_reg_grid,
                  n_iter=20,scoring='f1',cv=5,verbose=True)
#n_iter: is the number of random combinations of hyperparameters to try.
#cv: is the number of folds to use for cross-validation during the search.
#verbose: is a flag that controls the verbosity of the output during the search.

rs_log_reg.fit(X,Y)

rs_log_reg.best_score_

rs_log_reg.best_params_

"""# SVM"""

from sklearn import svm
model = svm.SVC()
model_val(model,X,Y)

""" Hyperparameter Tuning"""

svc_grid = {'C':[0.25,0.50,0.75,1],"kernel":["linear"]}

rs_svc=RandomizedSearchCV(svm.SVC(),
                  param_distributions=svc_grid,
                   cv=5,
                   n_iter=20,
                   scoring='f1',
                  verbose=True)

rs_svc.fit(X,Y)

rs_svc.best_score_

rs_svc.best_params_

"""# Decision Tree"""

from sklearn.tree import DecisionTreeClassifier
model = DecisionTreeClassifier()
model_val(model,X,Y)

"""# Save The Model"""

log_reg =RandomizedSearchCV(LogisticRegression(),
                   param_distributions=log_reg_grid,
                  n_iter=20,cv=5,verbose=True)

log_reg.fit(X,Y)

import joblib

joblib.dump(log_reg, 'loan_status_predict')

model = joblib.load('loan_status_predict')

df = pd.DataFrame({
    'Gender':1, #Male
    'Married':1, #Yes
    'Dependents':0, #can take 0,1,2,3
    'Education':0, #Graduated
    'Self_Employed':0, #No
    'ApplicantIncome':2000,
    'CoapplicantIncome':0.0,
    'LoanAmount':100, #in thousands 100,000$
    'Loan_Amount_Term':80, #in months
    'Credit_History':0, #No
    'Property_Area':1 #Rural=0 , SemiUrban=1 , Urban=2
},index=[0])

df = pd.DataFrame(df, columns=['Married', 'ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Credit_History'])

result = model.predict(df)

if result==1:
    print("Loan Approved")
else:
    print("Loan Not Approved")

"""# GUI by Tkinter"""

import tkinter as tk
from tkinter import ttk
import joblib
import pandas as pd

def toggle_mode():
    if mode_switch.instate(["selected"]):
        style.theme_use("forest-light")
    else:
        style.theme_use("forest-dark")

def validate_fields(*args):
    if (Genger_combobox.get() != "" and Married_variable.get() != "" and Dependents_spinbox.get() != "Number of dependents"
            and Education_combobox.get() != "" and Self_Employed_variable.get() != "" and ApplicantIncome_entry.get() != "Applicant_Income"
            and CoapplicantIncome_entry.get() != "CoapplicantIncome" and LoanAmount_entry.get() != "LoanAmount"
            and Loan_Amount_Term_entry.get() != "Loan_Amount_Term" and Credit_History_variable.get() != ""
            and Property_Area_combobox.get() != ""):
        button.config(state="normal")
    else:
        button.config(state="disabled")
def prediction():
    gender = 0 if "Female" else 1
    married = 1 if Married_variable.get() else 0
    dependents = Dependents_spinbox.get()
    if dependents == "Number of dependents":
        print("Falied")
    else:
        dependents = int(dependents)
    education = 0 if "Graduate" else 1
    self_employed = 1 if Self_Employed_variable.get() else 0
    applicant_income = float(ApplicantIncome_entry.get())
    coapplicant_income = float(CoapplicantIncome_entry.get())
    loan_amount = float(LoanAmount_entry.get())
    loan_amount_term = float(Loan_Amount_Term_entry.get())
    credit_history = 1 if Credit_History_variable.get() else 0
    property_area = 0 if Property_Area_combobox.get() == "Rural" else 1 if Property_Area_combobox.get() == "Semiurban" else 2

    model = joblib.load('loan_status_predict')
    df = pd.DataFrame({
        'Gender': gender,
        'Married': married,
        'Dependents': dependents,
        'Education': education,
        'Self_Employed': self_employed,
        'ApplicantIncome': applicant_income,
        'CoapplicantIncome': coapplicant_income,
        'LoanAmount': loan_amount,
        'Loan_Amount_Term': loan_amount_term,
        'Credit_History': credit_history,
        'Property_Area': property_area
    }, index=[0])
    df = pd.DataFrame(df, columns=['Married', 'ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Credit_History'])
    result = model.predict(df)

    if result == 1:
        prediction_label.configure(text="Loan approved")
    else:
        prediction_label.configure(text="Loan Not Approved")

root = tk.Tk()
style = ttk.Style(root)

root.tk.call("source", "Forest-ttk-theme-master/forest-light.tcl")
root.tk.call("source", "Forest-ttk-theme-master/forest-dark.tcl")
style.theme_use("forest-dark")


Genger_combobox_list=["Female","Male"]
Education_combobox_list=["Graduate","Not Graduate"]
Property_Area_combobox_list=["Rural","Semiurban","Urban"]
frame = ttk.Frame(root)
frame.pack()
widget_frame=ttk.LabelFrame(frame,text="Insert Data")
widget_frame.grid(row=0, column=0, padx=40, pady=40)

Gender_variable=tk.Variable
Genger_combobox=ttk.Combobox(widget_frame,values=Genger_combobox_list,textvariable=Gender_variable)
Genger_combobox.grid(row=0,column=0, padx=8, pady=8,sticky="ew")
Genger_combobox.current(0)

Married_variable=tk.BooleanVar()
Married_chechbutton=ttk.Checkbutton(widget_frame,text="Married",variable=Married_variable)
Married_chechbutton.grid(row= 1, column=0 ,padx=8, pady=8, sticky="nsew")#expanded in north-south-...all directions

Dependents_spinbox=ttk.Spinbox(widget_frame,from_=0,to=3)
Dependents_spinbox.grid(row=2 , column=0 ,padx=8,pady=8, sticky="ew")
Dependents_spinbox.insert(0,"Number of dependents") #to make placeholder


Education_variable=tk.Variable
Education_combobox=ttk.Combobox(widget_frame,values=Education_combobox_list,textvariable=Education_variable)
Education_combobox.grid(row=3,column=0,padx=8, pady=8,sticky="ew")
Education_combobox.current(0)

Self_Employed_variable=tk.BooleanVar()
Self_Employed_chechbutton=ttk.Checkbutton(widget_frame,text="Self_Employed",variable=Self_Employed_variable)
Self_Employed_chechbutton.grid(row=4 , column=0 ,padx=5,pady=(0,5), sticky="nsew")


ApplicantIncome_entry=ttk.Entry(widget_frame)
ApplicantIncome_entry.insert(5,"Applicant_Income") #to make placeholder
ApplicantIncome_entry.bind("<FocusIn>",lambda e:ApplicantIncome_entry.delete('0','end')) #to clear placeholder on focusIn
ApplicantIncome_entry.grid(padx=8, pady=8,sticky="ew")

CoapplicantIncome_entry=ttk.Entry(widget_frame)
CoapplicantIncome_entry.insert(6,"CoapplicantIncome") #to make placeholder
CoapplicantIncome_entry.bind("<FocusIn>",lambda e:CoapplicantIncome_entry.delete('0','end')) #to clear placeholder on focusIn
CoapplicantIncome_entry.grid(padx=8, pady=8,sticky="ew")

LoanAmount_entry=ttk.Entry(widget_frame)
LoanAmount_entry.insert(7,"LoanAmount(thousands)") #to make placeholder
LoanAmount_entry.bind("<FocusIn>",lambda e:LoanAmount_entry.delete('0','end')) #to clear placeholder on focusIn
LoanAmount_entry.grid(padx=8, pady=8,sticky="ew")


Loan_Amount_Term_entry=ttk.Entry(widget_frame)
Loan_Amount_Term_entry.insert(8,"Loan_Amount_Term") #to make placeholder
Loan_Amount_Term_entry.bind("<FocusIn>",lambda e:Loan_Amount_Term_entry.delete('0','end')) #to clear placeholder on focusIn
Loan_Amount_Term_entry.grid(padx=8, pady=8,sticky="ew")

Credit_History_variable=tk.BooleanVar()
Credit_History_chechbutton=ttk.Checkbutton(widget_frame,text="Credit_History",variable=Credit_History_variable)
Credit_History_chechbutton.grid(padx=8, pady=8,row=9 , column=0 , sticky="nsew")

Property_Area_variable=tk.Variable
Property_Area_combobox=ttk.Combobox(widget_frame,values=Property_Area_combobox_list,textvariable=Property_Area_variable)
Property_Area_combobox.grid(padx=8, pady=8,row=10,column=0,sticky="ew")
Property_Area_combobox.current(0)

Genger_combobox.bind("<FocusOut>", validate_fields)
Married_chechbutton.bind("<FocusOut>", validate_fields)
Dependents_spinbox.bind("<FocusOut>", validate_fields)
Education_combobox.bind("<FocusOut>", validate_fields)
Self_Employed_chechbutton.bind("<FocusOut>", validate_fields)
ApplicantIncome_entry.bind("<FocusOut>", validate_fields)
CoapplicantIncome_entry.bind("<FocusOut>", validate_fields)
LoanAmount_entry.bind("<FocusOut>", validate_fields)
Loan_Amount_Term_entry.bind("<FocusOut>", validate_fields)
Credit_History_chechbutton.bind("<FocusOut>", validate_fields)
Property_Area_combobox.bind("<FocusOut>", validate_fields)


button=ttk.Button(widget_frame,text="Predict",command=prediction,state="disabled")
button.grid(padx=8, pady=8,row=11,column=0,sticky="ew")

separator=ttk.Separator(widget_frame)
separator.grid(padx=20,pady=10,row=12,column=0,sticky="ew")

mode_switch=ttk.Checkbutton( widget_frame,text="Mode",style="Switch",command=toggle_mode)
mode_switch.grid(padx=5,pady=5,row=13,column=0,sticky="nsew")

predictionFrame=ttk.LabelFrame(frame,text="Prediction")
predictionFrame.grid(row=0,column=1,pady=10)

prediction_label = ttk.Label(predictionFrame, text="Prediction will appear here",padding=10)
prediction_label.grid(row=1, column=1, sticky="w")

root.mainloop()

"""# Conclusion:
In conclusion, we applied preprocessing techniques such as dealing with missing values, encoding categorical variables, and feature scaling. We also performed analysis and visualizations to understand the correlation between features. We trained three models, Logistic Regression, SVM, and Decision Tree, evaluated them on various metrics, and found that Logistic Regression performed the best with the highest accuracy and better precision and recall scores. Overall, the project achieved the intended goal of automating the loan eligibility process based on customer details.
"""